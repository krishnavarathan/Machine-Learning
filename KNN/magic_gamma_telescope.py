# -*- coding: utf-8 -*-
"""MAGIC Gamma Telescope

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QmrIN9DBCFoXnR0dHXiqyTTFdSzN1swF
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

cols=['fLength', 'fWidth', 'fSize', 'fConc','fConc1' ,'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']
df=pd.read_csv('KNN/magic04.data', names=cols)
df.head()

# These are the predicted final values
pd.unique(df.values[:,-1])

# converting the g and h into 1 and 0
df['class']= (df['class'] == 'g').astype(int)

df['class'].unique()

# Plotting them into plt
for label in cols[:-1]:
  plt.hist(df[df['class']==1][label], color='blue', label='gamma', alpha=0.7, density=True)
  plt.hist(df[df['class']==0][label], color='red', label='handron', alpha=0.7, density=True)
  plt.title(label)
  plt.ylabel('Probability')
  plt.xlabel(label)
  plt.legend()
  plt.show()

"""## Train Validation Test Dat-Set"""

df.columns

# np.split() => split the data horizandle-wise
# .sample(frc=1) => it shuffle the over data-set by rows
train, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])
# print(test, valid)

# hardon value very low compare to gamma
# Use the index -1 instead of the name 'class'
print(len(train[train[:, -1] == 1]))
print(len(train[train[:, -1] == 0]))

# print(len(train[train['class']==1])) # gamma
# print(len(train[train['class']==0])) # hardon

print(train.shape)
print(valid.shape)
print(test.shape)

"""Normalize and Pre-Processing the data-set using **StandardScaler() and RandomOverSampler()**"""

# dataframe.columns[:-1] => Taking the all columns expect last one('class')
# dataframe[dataframe.columns[:-1]].values => taking the all column values
def scale_dataset(dataframe, over_sample=False):
  X=dataframe[dataframe.columns[:-1]].values
  y=dataframe[dataframe.columns[-1]].values
  # print("X size:", len(X))
  # print("y size:", len(y))

  # Normalize the feature input data
  scaler=StandardScaler()
  X = scaler.fit_transform(X)

  if over_sample:
    ros=RandomOverSampler()
    X, y=ros.fit_resample(X, y)
    # print("X size:", len(X))
    # print("y size:", len(y))


  # fitting the resample data which is X and
  data = np.hstack((X, np.reshape(y, (-1, 1)))) # changing 1D array into 2-D array (-1) tells no.of rows, (1) tells one no.of column

  return data, X, y

"""Passing the train, valid, test data-sets and
stored the output corresponding variables
"""

train, X_train, y_train = scale_dataset(train, over_sample=True)
valid, X_valid, y_valid = scale_dataset(valid, over_sample=False)
test, X_test, y_test = scale_dataset(test, over_sample=False)

print(train.shape)
print(X_train.shape)
print(y_train.shape)

print(y_train)
print(y_valid)
print(y_test)

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

# .fit() does in KNN
# Unlike other algorithms, KNN does not calculate equations.
# It simply stores the training data in memory.

knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X_train, y_train)

# Passing the training data-set as input feautures
y_pred = knn_model.predict(X_train)

print(classification_report(y_train, y_pred))
print("Accuracy:", accuracy_score(y_train, y_pred))

# print(type(y_pred))
# print(type(y_train))
# print(len(y_pred))
# print(len(y_train))

# Observing the output
print(len(X_train))
print(np.unique(y_pred, return_counts=True))
print(np.unique(y_train, return_counts=True))